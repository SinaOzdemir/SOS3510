[["text-as-data-obtaining-text.html", "Chapter 6 Text-as-data: obtaining text: 6.1 Machine readable vs non-machine readable text 6.2 Manual download and manual read 6.3 Web-scraping 6.4 Application programming interfaces (APIs)", " Chapter 6 Text-as-data: obtaining text: Text as data has gained quite a traction in political science and many other social science disciplines in the recent years. This partly due to the abundance of text we can access and partly the technology that allows us collect and analyse large amounts of text. We will take a look at a few common ways to obtain text from various sources and how to read it in R so that we can work on it further. 6.1 Machine readable vs non-machine readable text We use most often machine readable text when we work with text-as-data. Machine readable text refers to the fact that the text is digitized. It has a binary representation so that computers can process it as information. Most texts we get from online sources are machine readable. These can easily be loaded into R or other software to work further. Non-machine readable texts are those that are not yet put into binary format. Texts that are still on paper are good examples. Another problem might be that the text is digitized but in image format. For example, when you scan something, it is actually an image not a text. Recent developments in Optical Character Recognition (OCR) can help us convert these to machine readable text. While it is quite easy to do this, it is often sub-optimal. OCR is essentially an image classification machine learning that looks at the picture and tries to guess what characters there are in the image. So it always comes with a margin of error. Lets take a look at how we can do this. We will use the image below for illustration ## here() starts at C:/Users/sinaoz/OneDrive - NTNU/Work/Pflictarbeit/Teaching/SOS3515_2022/automated_text_analysis library(tesseract) OCR_engine&lt;- tesseract(language = &quot;eng&quot;) machine_readable_text&lt;- tesseract::ocr(image = here(&quot;testocr.png&quot;),engine = OCR_engine) print(machine_readable_text) ## [1] &quot;This is a lot of 12 point text to test the\\nocr code and see if it works on all types\\nof file format.\\n\\nThe quick brown dog jumped over the\\nlazy fox. The quick brown dog jumped\\nover the lazy fox. The quick brown dog\\njumped over the lazy fox. The quick\\nbrown dog jumped over the lazy fox.\\n&quot; Lets break down what we did there. First we load the package tesseract. This is the package for OCR engine that processes the image and extracts the text. Then we specify which language we will use with tesseract() function. Finally, we feed the image to the engine with tesseract::ocr() function. This extracts the text from the image. You can also use tesseract::ocr_data() to get a detailed dataset of words and prediction confidence if you want to work further. Finally, the image we used is a clean example. It is a black and white image of typewritten text (as opposed to handwritten). Not all text images behave so nicely. They could be colored, handwritten or very blurry. You can use magick package to clean the image, for example convert it to black and white, before feeding them into OCR. There is no recipe for such a task. Youll need to play around and figure out what works best for you. 6.2 Manual download and manual read While many things can be automated (including downloading images and texts), there is always the option of downloading the text manually. You can do this from a website like Hansard for the UK house of commons. These are just databases where you download a text. There are few things that would make your life a lot easier if you are manually downloading text. First and foremost, the file format. When you are downloading text online, pay attention to the format you are saving the file. .TXT files are easier to use in natural language processing than PDFs and PDFs are easier to use than word documents. The second thing you need to pay attention is the file encoding. You may have heard things like ASCII and UTF-8 before. These are systems a computer uses to identify and display letters, remember computers only understand numbers. Some characters have representation in their own system. For example, UTF-8 is one of the most versatile and common system to represent English alphabetical characters but it doesnt have ways to represent Norwegian letters. So if you save a Norwegian text in UTF-8 there is a good chance that there will be odd letter. Finally, it is important to pay attention to the file name when you are saving it. Special characters and white spaces can sometimes cause a problem in reading the file into R or other software. Lets see how we can read different file formats into R to work on them furher. The main package we use for this is textreadr. # Install the package, you need to do this only once #install.packages(&quot;textreadr&quot;) #Attach the package to the session, you need to this every time you start R library(textreadr) #reading txt files txt_file&lt;- read.table(file = here(&quot;lorem_ipsum.txt&quot;),header = F) head(txt_file$V1) ## [1] &quot;\\n\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Nullam suscipit lacinia neque, quis eleifend erat venenatis euismod. Aenean molestie, nisi id congue tincidunt, arcu nulla mollis quam, non eleifend ipsum odio non nibh. Ut ligula metus, tincidunt vel elit ut, hendrerit luctus enim. Aenean lacinia ac sapien sed pellentesque. Phasellus molestie imperdiet sodales. Fusce vehicula purus et dolor accumsan, vel sollicitudin orci maximus. Integer ac luctus lacus, vitae imperdiet purus. Nulla at rhoncus ligula. Nullam lacinia lacinia lorem, sit amet auctor urna suscipit rutrum. Nullam eu porta libero. Cras vel urna vitae odio rutrum hendrerit a in eros. Morbi eu facilisis nibh. Praesent lacinia commodo ultricies. Nunc quis diam auctor, semper sapien fermentum, porta odio. Pellentesque odio lacus, dictum ut iaculis aliquet, vehicula vitae dui.\\n\\nMaecenas faucibus viverra aliquam. Morbi tellus ipsum, bibendum ac fermentum ut, volutpat quis dui. Phasellus hendrerit, ipsum quis accumsan condimentum, ex tellus laoreet diam, nec finibus massa velit quis tellus. Sed maximus scelerisque ex id elementum. Mauris vel lacinia lorem. Vivamus auctor pretium metus, at placerat ex venenatis in. Donec mauris est, ultricies sed gravida sed, volutpat ut augue.\\n\\nDonec non scelerisque purus, sed dapibus ante. Vestibulum dictum enim dignissim felis condimentum, ac viverra nisi cursus. Cras egestas magna vitae purus ultricies malesuada. Suspendisse euismod, enim non venenatis tempus, velit odio cursus ipsum, a faucibus dui quam quis ante. Aenean tristique in eros non dictum. In ornare consequat quam eu mollis. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Donec at euismod turpis. Phasellus a feugiat magna. Ut rutrum ornare felis consequat rutrum. Cras hendrerit tellus nec dui malesuada, a blandit felis lacinia. Nullam sed mattis metus. Sed dictum massa quis leo molestie varius.\\n\\nPraesent egestas nisi erat. Morbi rutrum congue lacinia. Donec lobortis nunc vitae semper vulputate. Quisque ullamcorper sit amet eros eget gravida. Cras molestie augue quis eros blandit efficitur. Aenean id magna tempor, lobortis nisi sed, placerat eros. Quisque imperdiet elit sed lorem venenatis, nec porttitor risus consectetur. In hac habitasse platea dictumst. Aliquam venenatis urna ac varius tincidunt. Vestibulum sodales, sem a vulputate sagittis, mauris lorem facilisis ante, non convallis leo magna id eros.\\n\\nMaecenas mollis non orci mollis feugiat. Vivamus scelerisque ut velit sed bibendum. Cras auctor cursus turpis a posuere. Integer maximus mollis arcu. In convallis, lacus sit amet pellentesque consectetur, nisl erat lobortis leo, sed venenatis tortor dolor eget arcu. Praesent vitae nisi magna. Vestibulum ultricies elementum rutrum. Donec posuere eleifend sodales. Nunc at erat ut sem gravida semper et vitae lectus. &quot; #reading pdf files pdf_file&lt;- read_pdf(file = here(&quot;lorem_ipsum.pdf&quot;)) head(pdf_file$text) ## [1] &quot;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nullam suscipit lacinia neque, quis eleifend\\nerat venenatis euismod. Aenean molestie, nisi id congue tincidunt, arcu nulla mollis quam, non\\neleifend ipsum odio non nibh. Ut ligula metus, tincidunt vel elit ut, hendrerit luctus enim. Aenean\\nlacinia ac sapien sed pellentesque. Phasellus molestie imperdiet sodales. Fusce vehicula purus et\\ndolor accumsan, vel sollicitudin orci maximus. Integer ac luctus lacus, vitae imperdiet purus. Nulla at\\nrhoncus ligula. Nullam lacinia lacinia lorem, sit amet auctor urna suscipit rutrum. Nullam eu porta\\nlibero. Cras vel urna vitae odio rutrum hendrerit a in eros. Morbi eu facilisis nibh. Praesent lacinia\\ncommodo ultricies. Nunc quis diam auctor, semper sapien fermentum, porta odio. Pellentesque odio\\nlacus, dictum ut iaculis aliquet, vehicula vitae dui.\\n\\n\\n\\nMaecenas faucibus viverra aliquam. Morbi tellus ipsum, bibendum ac fermentum ut, volutpat quis\\ndui. Phasellus hendrerit, ipsum quis accumsan condimentum, ex tellus laoreet diam, nec finibus\\nmassa velit quis tellus. Sed maximus scelerisque ex id elementum. Mauris vel lacinia lorem. Vivamus\\nauctor pretium metus, at placerat ex venenatis in. Donec mauris est, ultricies sed gravida sed,\\nvolutpat ut augue.\\n\\n\\n\\nDonec non scelerisque purus, sed dapibus ante. Vestibulum dictum enim dignissim felis\\ncondimentum, ac viverra nisi cursus. Cras egestas magna vitae purus ultricies malesuada.\\nSuspendisse euismod, enim non venenatis tempus, velit odio cursus ipsum, a faucibus dui quam quis\\nante. Aenean tristique in eros non dictum. In ornare consequat quam eu mollis. Lorem ipsum dolor\\nsit amet, consectetur adipiscing elit. Donec at euismod turpis. Phasellus a feugiat magna. Ut rutrum\\nornare felis consequat rutrum. Cras hendrerit tellus nec dui malesuada, a blandit felis lacinia. Nullam\\nsed mattis metus. Sed dictum massa quis leo molestie varius.\\n\\n\\n\\nPraesent egestas nisi erat. Morbi rutrum congue lacinia. Donec lobortis nunc vitae semper vulputate.\\nQuisque ullamcorper sit amet eros eget gravida. Cras molestie augue quis eros blandit efficitur.\\nAenean id magna tempor, lobortis nisi sed, placerat eros. Quisque imperdiet elit sed lorem\\nvenenatis, nec porttitor risus consectetur. In hac habitasse platea dictumst. Aliquam venenatis urna\\nac varius tincidunt. Vestibulum sodales, sem a vulputate sagittis, mauris lorem facilisis ante, non\\nconvallis leo magna id eros.\\n\\n\\n\\nMaecenas mollis non orci mollis feugiat. Vivamus scelerisque ut velit sed bibendum. Cras auctor\\ncursus turpis a posuere. Integer maximus mollis arcu. In convallis, lacus sit amet pellentesque\\nconsectetur, nisl erat lobortis leo, sed venenatis tortor dolor eget arcu. Praesent vitae nisi magna.\\nVestibulum ultricies elementum rutrum. Donec posuere eleifend sodales. Nunc at erat ut sem\\ngravida semper et vitae lectus.&quot; #reading word files doc_file&lt;- read_docx(file = here(&quot;lorem_ipsum.docx&quot;)) head(doc_file) ## [1] &quot;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nullam suscipit lacinia neque, quis eleifend erat venenatis euismod. Aenean molestie, nisi id congue tincidunt, arcu nulla mollis quam, non eleifend ipsum odio non nibh. Ut ligula metus, tincidunt vel elit ut, hendrerit luctus enim. Aenean lacinia ac sapien sed pellentesque. Phasellus molestie imperdiet sodales. Fusce vehicula purus et dolor accumsan, vel sollicitudin orci maximus. Integer ac luctus lacus, vitae imperdiet purus. Nulla at rhoncus ligula. Nullam lacinia lacinia lorem, sit amet auctor urna suscipit rutrum. Nullam eu porta libero. Cras vel urna vitae odio rutrum hendrerit a in eros. Morbi eu facilisis nibh. Praesent lacinia commodo ultricies. Nunc quis diam auctor, semper sapien fermentum, porta odio. Pellentesque odio lacus, dictum ut iaculis aliquet, vehicula vitae dui.&quot; ## [2] &quot;Maecenas faucibus viverra aliquam. Morbi tellus ipsum, bibendum ac fermentum ut, volutpat quis dui. Phasellus hendrerit, ipsum quis accumsan condimentum, ex tellus laoreet diam, nec finibus massa velit quis tellus. Sed maximus scelerisque ex id elementum. Mauris vel lacinia lorem. Vivamus auctor pretium metus, at placerat ex venenatis in. Donec mauris est, ultricies sed gravida sed, volutpat ut augue.&quot; ## [3] &quot;Donec non scelerisque purus, sed dapibus ante. Vestibulum dictum enim dignissim felis condimentum, ac viverra nisi cursus. Cras egestas magna vitae purus ultricies malesuada. Suspendisse euismod, enim non venenatis tempus, velit odio cursus ipsum, a faucibus dui quam quis ante. Aenean tristique in eros non dictum. In ornare consequat quam eu mollis. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Donec at euismod turpis. Phasellus a feugiat magna. Ut rutrum ornare felis consequat rutrum. Cras hendrerit tellus nec dui malesuada, a blandit felis lacinia. Nullam sed mattis metus. Sed dictum massa quis leo molestie varius.&quot; ## [4] &quot;Praesent egestas nisi erat. Morbi rutrum congue lacinia. Donec lobortis nunc vitae semper vulputate. Quisque ullamcorper sit amet eros eget gravida. Cras molestie augue quis eros blandit efficitur. Aenean id magna tempor, lobortis nisi sed, placerat eros. Quisque imperdiet elit sed lorem venenatis, nec porttitor risus consectetur. In hac habitasse platea dictumst. Aliquam venenatis urna ac varius tincidunt. Vestibulum sodales, sem a vulputate sagittis, mauris lorem facilisis ante, non convallis leo magna id eros.&quot; ## [5] &quot;Maecenas mollis non orci mollis feugiat. Vivamus scelerisque ut velit sed bibendum. Cras auctor cursus turpis a posuere. Integer maximus mollis arcu. In convallis, lacus sit amet pellentesque consectetur, nisl erat lobortis leo, sed venenatis tortor dolor eget arcu. Praesent vitae nisi magna. Vestibulum ultricies elementum rutrum. Donec posuere eleifend sodales. Nunc at erat ut sem gravida semper et vitae lectus.&quot; textreadr also offers functions to read other file formats such as .rtf and .odt. 6.3 Web-scraping Another possible source for text is websites on internet. We can scrape pretty much all websites but some are easier than others. If a website is static, it is fairly easy to get the text. Conversely, dynamic websites are a little harder. Static websites are those that have everything in one html file so when you go to the website the content you want is already provided. Dynamic websites, on the other hand, have information in some sort of database and call them based on demand. Websites that you need to click places to get the text are usually dynamic websites. Web scraping is essentially just reading a whole website into R and then selecting the relevant parts. In an html page, different parts of a page are marked with html tags, and these are what we use to specify what we want to extract. To illustrate this, lets assume that we want to scrape Storting elections in 2021 on wikipedia. If we want the information about the turnout in the table, we need to look for its html tag in the page. In this case it is table class = wikitable ib-legis-elect-results plainrowheaders as shown in the figure below. You can find this by right-cliking on the element and selecting inspect in the drop down menu. There are two tools that make this whole process a lot easier. First is the css selector gadget. This is a browser extension that extracts the html tag for you by simply left clicking on the element. The extention is available in chorme and firefox but it works better on chrome browser. It looks like the following image when you use the selector gadget extention. The other tool we need is a package called rvest. This package helps us to read the html page, extract the information using the tag and put it into a usable dataset format. here is the example code for scraping Norwegian elections: #install.package(&quot;rvest&quot;) library(rvest) ## ## Attaching package: &#39;rvest&#39; ## The following object is masked from &#39;package:textreadr&#39;: ## ## read_html website&lt;- read_html(x = &quot;https://en.wikipedia.org/wiki/2021_Norwegian_parliamentary_election&quot;) party&lt;- website %&gt;% html_elements(css = &quot;.plainrowheaders th a&quot;) %&gt;% html_text(trim = T) leaders&lt;- website %&gt;% html_elements(css = &quot;.plainrowheaders th+ td&quot;) %&gt;% html_text(trim = T) percentages&lt;- website %&gt;% html_elements(css = &quot;.ib-legis-elect-percent&quot;) %&gt;% html_text(trim = T) %&gt;% as.numeric() seats&lt;- website %&gt;% html_elements(css = &quot;.ib-legis-elect-percent+ .ib-legis-elect-seats&quot;) %&gt;% html_text(trim = T) %&gt;% as.numeric() change &lt;- website %&gt;% html_elements(css = &quot;.nowrap.ib-legis-elect-seats&quot;) %&gt;% html_text(trim = T) %&gt;% as.numeric() storting_election_results&lt;- data.frame(party = party, leaders = leaders, percent = percentages, n_seats = seats, change = change) head(storting_election_results) ## party leaders percent n_seats change ## 1 Labour Jonas Gahr Støre 26.3 48 -1 ## 2 Conservative Erna Solberg 20.4 36 -9 ## 3 Centre Trygve Slagsvold Vedum 13.5 28 9 ## 4 Progress Sylvi Listhaug 11.6 21 -6 ## 5 Socialist Left Audun Lysbakken 7.6 13 2 ## 6 Red Bjørnar Moxnes 4.7 8 7 Lets break down the code above. First we read the html website with read_html. Then, using selector gadget we identify tags for elements we want and extract them from the html. In this particular case we had to individually extract information on parties, leaders etc. because of the website design. There were too many overlapping html tags and attributes, so for example if we just extracted html elements a, we would also get other irrelevant things like links in the text body. Finally we finally put everything into one dataset with data.frame() function. One last thing about web scraping is whether it is allowed or not. You should check the robot.txt of the webpage (eg.www.webpage.com/robot.txt) to see if you are allowed to do so. 6.4 Application programming interfaces (APIs) Application programming interfaces are intermediaries between a user and a database. You can think of them as digital librarians. When we want some information from a database with an API, we essentially make a request to this digital librarian which in turn gives us the dataset. Therefore, they are often preferable to webscraping as they can provide a clean, targeted and formated dataset. There are several key points to know about APIs. First of all, most APIs require some form key. These keys indicate that you are allowed to access data sets in the database. These keys are often obtained by creating an account on a webpage and requesting a key. The second important point is that most APIs provide data sets in JSON format. JSON is a hierarchical dataset format consisting of list of lists with name-element combinations. While JSON offers a light weight and structured dataset, it still takes some data wrangling to turn it into tabular data. Lastly, some APIs have dedicated R packages. These packages automate often simplify the process of building requests and data sets, so it is generally advisable to look for a package before using raw API. For this example we would like to get the party manifestos of Right (H) party of Norway between 1990 and 2017 Lets illustrate the use of API with http calls and a package using Manifesto project. library(pacman) packs&lt;- c(&quot;httr&quot;,&quot;jsonlite&quot;) p_load(char = packs) base_link&lt;- &quot;https://manifesto-project.wzb.eu/api/v1/&quot; api_key&lt;- paste0(&quot;api_key=&quot;,&quot;5a937a227a2ca01a97e5e017bb77720f&quot;) data_call&lt;- &quot;get_core?&quot; #create the request for data dataset_call&lt;- paste0(base_link,data_call,api_key,&quot;&amp;&quot;,&quot;key=&quot;,&quot;MPDS2021a&quot;,&quot;&amp;&quot;,&quot;kind=&quot;,&quot;csv&quot;) #make the call dataset_raw&lt;- httr::GET(dataset_call) #convert the raw encoding to character encoding dataset_char&lt;- rawToChar(dataset_raw$content) #convert json to data frame dataset_json&lt;- fromJSON(txt = dataset_char,flatten = T,simplifyDataFrame = F) #wrangle dataset into a workable format data_colnames&lt;- dataset_json[1,] colnames(dataset_json)&lt;-data_colnames dataset&lt;-dataset_json[-1,] %&gt;% as.data.frame() head(dataset) ## country countryname oecdmember eumember edate date party ## 1 11 Sweden 0 0 17/09/1944 194409 11220 ## 2 11 Sweden 0 0 17/09/1944 194409 11320 ## 3 11 Sweden 0 0 17/09/1944 194409 11420 ## 4 11 Sweden 0 0 17/09/1944 194409 11620 ## 5 11 Sweden 0 0 17/09/1944 194409 11810 ## 6 11 Sweden 0 0 19/09/1948 194809 11220 ## partyname partyabbrev parfam coderid manual coderyear ## 1 Communist Party of Sweden SKP 20 117 0 1983 ## 2 Social Democratic Labour Party SAP 30 117 0 1983 ## 3 Peoples Party FP 40 117 0 1983 ## 4 Right Party 60 117 0 1983 ## 5 Agrarian Party 80 117 0 1983 ## 6 Communist Party of Sweden SKP 20 117 0 1983 ## testresult testeditsim pervote voteest presvote absseat totseats progtype ## 1 &lt;NA&gt; &lt;NA&gt; 10.3 0 &lt;NA&gt; 15 230 1 ## 2 &lt;NA&gt; &lt;NA&gt; 46.5 0 &lt;NA&gt; 115 230 1 ## 3 &lt;NA&gt; &lt;NA&gt; 12.9 0 &lt;NA&gt; 26 230 1 ## 4 &lt;NA&gt; &lt;NA&gt; 15.8 0 &lt;NA&gt; 39 230 1 ## 5 &lt;NA&gt; &lt;NA&gt; 13.6 0 &lt;NA&gt; 35 230 1 ## 6 &lt;NA&gt; &lt;NA&gt; 6.312 0 &lt;NA&gt; 8 230 1 ## datasetorigin corpusversion total peruncod per101 per102 per103 per104 per105 ## 1 41 52 &lt;NA&gt; 1.9 0 0 0 0 ## 2 41 90 &lt;NA&gt; 1.1 0 0 2.2 0 ## 3 41 63 &lt;NA&gt; 3.2 0 0 7.9 0 ## 4 41 57 &lt;NA&gt; 1.8 0 0 1.8 0 ## 5 41 21 42.857 0 0 0 4.762 0 ## 6 10 50 &lt;NA&gt; 2 0 10 0 0 ## per106 per107 per108 per109 per110 per201 per202 per203 per204 ## 1 1.9 0 0 0 0 9.6 0 1.9 0 ## 2 5.600000000000001 4.4 0 0 0 3.3 1.1 0 0 ## 3 1.6 3.2 0 0 0 12.7 6.4 0 0 ## 4 0 3.5 0 0 0 7 0 0 0 ## 5 4.762 0 0 0 0 0 0 0 0 ## 6 10 0 0 0 0 4 10 0 0 ## per301 per302 per303 per304 per305 per401 per402 per403 per404 ## 1 0 0 0 0 0 0 0 0 0 ## 2 0 0 0 0 0 0 0 0 3.3 ## 3 3.2 0 1.6 0 0 6.4 3.2 1.6 1.6 ## 4 1.8 0 1.8 0 0 17.5 0 1.8 0 ## 5 0 0 0 0 0 9.524000000000001 0 0 0 ## 6 0 0 4 0 0 0 0 0 0 ## per405 per406 per407 per408 per409 per410 per411 per412 per413 ## 1 0 0 0 3.8 0 0 0 1.9 0 ## 2 0 0 0 11.1 0 3.3 0 0 0 ## 3 0 0 0 3.2 0 0 0 0 0 ## 4 0 0 0 1.8 0 0 0 0 0 ## 5 0 0 0 0 0 0 0 0 0 ## 6 0 0 0 8 0 0 0 8 0 ## per414 per415 per416 per501 per502 per503 per504 per505 ## 1 1.9 0 0 0 0 0 0 0 ## 2 2.2 0 0 0 0 5.600000000000001 27.8 0 ## 3 0 0 0 0 0 1.6 12.7 0 ## 4 5.3 0 0 0 0 5.3 5.3 0 ## 5 9.524000000000001 0 0 0 0 0 0 0 ## 6 2 0 0 0 0 6 12 0 ## per506 per507 per601 per602 per603 per604 per605 per606 per607 per608 per701 ## 1 0 0 0 0 0 0 0 0 0 0 0 ## 2 3.3 0 0 0 0 0 0 0 0 0 0 ## 3 4.8 0 0 0 6.4 0 0 4.8 4.8 0 0 ## 4 7 0 3.5 0 10.5 0 0 0 0 0 0 ## 5 0 0 4.762 0 0 0 0 0 0 0 0 ## 6 2 0 0 0 0 0 0 2 0 0 0 ## per702 per703 per704 per705 per706 per1011 per1012 per1013 per1014 ## 1 0 0 0 0 0 0 0 0 0 ## 2 0 4.4 0 0 1.1 0 0 0 0 ## 3 0 4.8 0 0 0 0 0 0 0 ## 4 0 3.5 8.800000000000001 0 3.5 0 0 0 0 ## 5 0 23.81 0 0 0 0 0 0 0 ## 6 0 0 0 0 4 0 0 0 0 ## per1015 per1016 per1021 per1022 per1023 per1024 per1025 per1026 per1031 ## 1 0 0 0 0 0 0 0 0 0 ## 2 0 0 0 0 0 0 0 0 0 ## 3 0 0 0 0 0 0 0 0 0 ## 4 0 0 0 0 0 0 0 0 0 ## 5 0 0 0 0 0 0 0 0 0 ## 6 0 0 0 0 0 0 0 0 0 ## per1032 per1033 per2021 per2022 per2023 per2031 per2032 per2033 per2041 ## 1 0 0 0 0 0 0 0 0 0 ## 2 0 0 0 0 0 0 0 0 0 ## 3 0 0 0 0 0 0 0 0 0 ## 4 0 0 0 0 0 0 0 0 0 ## 5 0 0 0 0 0 0 0 0 0 ## 6 0 0 0 0 0 0 0 0 0 ## per3011 per3051 per3052 per3053 per3054 per3055 per4011 per4012 per4013 ## 1 0 0 0 0 0 0 0 0 0 ## 2 0 0 0 0 0 0 0 0 0 ## 3 0 0 0 0 0 0 0 0 0 ## 4 0 0 0 0 0 0 0 0 0 ## 5 0 0 0 0 0 0 0 0 0 ## 6 0 0 0 0 0 0 0 0 0 ## per4014 per4121 per4122 per4123 per4124 per4131 per4132 per5021 per5031 ## 1 0 0 0 0 0 0 0 0 0 ## 2 0 0 0 0 0 0 0 0 0 ## 3 0 0 0 0 0 0 0 0 0 ## 4 0 0 0 0 0 0 0 0 0 ## 5 0 0 0 0 0 0 0 0 0 ## 6 0 0 0 0 0 0 0 0 0 ## per5041 per5061 per6011 per6012 per6013 per6014 per6061 per6071 per6072 ## 1 0 0 0 0 0 0 0 0 0 ## 2 0 0 0 0 0 0 0 0 0 ## 3 0 0 0 0 0 0 0 0 0 ## 4 0 0 0 0 0 0 0 0 0 ## 5 0 0 0 0 0 0 0 0 0 ## 6 0 0 0 0 0 0 0 0 0 ## per6081 per7051 per7052 per7061 per7062 per103_1 per103_2 per201_1 per201_2 ## 1 0 0 0 0 0 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 2 0 0 0 0 0 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 3 0 0 0 0 0 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 4 0 0 0 0 0 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 5 0 0 0 0 0 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 6 0 0 0 0 0 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## per202_1 per202_2 per202_3 per202_4 per305_1 per305_2 per305_3 per305_4 ## 1 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 2 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 3 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 4 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 5 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 6 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## per305_5 per305_6 per416_1 per416_2 per601_1 per601_2 per602_1 per602_2 ## 1 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 2 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 3 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 4 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 5 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 6 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## per605_1 per605_2 per606_1 per606_2 per607_1 per607_2 per607_3 per608_1 ## 1 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 2 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 3 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 4 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 5 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 6 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## per608_2 per608_3 per703_1 per703_2 rile planeco markeco welfare ## 1 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 9.6 1.9 1.9 0 ## 2 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; -37.8 3.3 2.2 33.4 ## 3 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 9.5 3.2 6.4 14.3 ## 4 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 28 1.8 22.8 10.6 ## 5 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 23.81 0 19.048 0 ## 6 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; -44 8 2 18 ## intpeace datasetversion id_perm ## 1 1.9 2021a JN1LZH ## 2 5.600000000000001 2021a CMR7F6 ## 3 1.6 2021a Z6OL6C ## 4 0 2021a YMKVN2 ## 5 4.762 2021a U4SCRD ## 6 10 2021a KU4SLH Lets break down the code. First we create the http link for API. This requires a bit http knowledge but here are few key things. API calls always have a base link to the API. Then we add the request to it, in this case it is get_core. We use question mark ? to indicate that we will start specifying arguments to the request. Then we use &amp; to combine multiple arguments. In the example above, we put MPDS2021a as the key argument (i.e name of the dataset) and csv for kind. We then send this request with httr::GET(). GET is one of the several ways to communicate with the API, you can take a look here for other methods. We then get the dataset from API in raw format. This is literally 1s and 0s, so it is no way usable at the moment. We convert them to character data in JSON format and then put it in a data-frame. Only then the dataset becomes useful. Now repeat the same thing with Manifesto projects dedicated R package. #install.packages(&quot;manifestoR&quot;) library(manifestoR) ## Zorunlu paket yükleniyor: NLP ## ## Attaching package: &#39;NLP&#39; ## The following object is masked from &#39;package:httr&#39;: ## ## content ## Zorunlu paket yükleniyor: tm ## When publishing work using the Manifesto Corpus, please make sure to cite it correctly and to give the identification number of the corpus version used for your analysis. ## ## You can print citation and version information with the function mp_cite(). ## ## Note that some of the scaling/analysis algorithms provided with this package were conceptually developed by authors referenced in the respective function documentation. Please also reference them when using these algorithms. mp_dataset&lt;- mp_maindataset(apikey = &quot;5a937a227a2ca01a97e5e017bb77720f&quot;,version = &quot;MPDS2021a&quot;) ## Connecting to Manifesto Project DB API... corpus version: 2021-1 head(mp_dataset) ## # A tibble: 6 x 174 ## country count~1 oecdm~2 eumem~3 edate date party party~4 party~5 parfam ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 11 Sweden 0 0 1944-09-17 194409 11220 Commun~ &quot;SKP&quot; 20 ## 2 11 Sweden 0 0 1944-09-17 194409 11320 Social~ &quot;SAP&quot; 30 ## 3 11 Sweden 0 0 1944-09-17 194409 11420 People~ &quot;FP&quot; 40 ## 4 11 Sweden 0 0 1944-09-17 194409 11620 Right ~ &quot;&quot; 60 ## 5 11 Sweden 0 0 1944-09-17 194409 11810 Agrari~ &quot;&quot; 80 ## 6 11 Sweden 0 0 1948-09-19 194809 11220 Commun~ &quot;SKP&quot; 20 ## # ... with 164 more variables: coderid &lt;dbl&gt;, manual &lt;dbl&gt;, coderyear &lt;dbl&gt;, ## # testresult &lt;dbl&gt;, testeditsim &lt;dbl&gt;, pervote &lt;dbl&gt;, voteest &lt;dbl&gt;, ## # presvote &lt;dbl&gt;, absseat &lt;dbl&gt;, totseats &lt;dbl&gt;, progtype &lt;dbl&gt;, ## # datasetorigin &lt;dbl&gt;, corpusversion &lt;chr&gt;, total &lt;dbl&gt;, peruncod &lt;dbl&gt;, ## # per101 &lt;dbl&gt;, per102 &lt;dbl&gt;, per103 &lt;dbl&gt;, per104 &lt;dbl&gt;, per105 &lt;dbl&gt;, ## # per106 &lt;dbl&gt;, per107 &lt;dbl&gt;, per108 &lt;dbl&gt;, per109 &lt;dbl&gt;, per110 &lt;dbl&gt;, ## # per201 &lt;dbl&gt;, per202 &lt;dbl&gt;, per203 &lt;dbl&gt;, per204 &lt;dbl&gt;, per301 &lt;dbl&gt;, ... ## # i Use `colnames()` to see all variable names As you can see, using the package reduced the workload to a single line of code. Using APIs via http communication (i.e raw) is often more tedious, so if you have the option for a package or a library always choose that one. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
